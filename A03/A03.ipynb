{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**A03. Word Sense**"
      ],
      "metadata": {
        "id": "aEkwY0RBIWzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "lEuzb2PoJAqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem1**\n",
        "\n",
        "##Use sorted() and set() to get a sorted list of tags used in the Brown corpus, removing duplicates."
      ],
      "metadata": {
        "id": "pUy6mAmtI25F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-yAsWZQID3D",
        "outputId": "b3a2979c-360e-417e-b1bc-d166bf6e9e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", \"''\", '(', '(-HL', ')', ')-HL', '*', '*-HL', '*-NC', '*-TL', ',', ',-HL', ',-NC', ',-TL', '--', '---HL', '.', '.-HL', '.-NC', '.-TL', ':', ':-HL', ':-TL', 'ABL', 'ABN', 'ABN-HL', 'ABN-NC', 'ABN-TL', 'ABX', 'AP', 'AP$', 'AP+AP-NC', 'AP-HL', 'AP-NC', 'AP-TL', 'AT', 'AT-HL', 'AT-NC', 'AT-TL', 'AT-TL-HL', 'BE', 'BE-HL', 'BE-TL', 'BED', 'BED*', 'BED-NC', 'BEDZ', 'BEDZ*', 'BEDZ-HL', 'BEDZ-NC', 'BEG', 'BEM', 'BEM*', 'BEM-NC', 'BEN', 'BEN-TL', 'BER', 'BER*', 'BER*-NC', 'BER-HL', 'BER-NC', 'BER-TL', 'BEZ', 'BEZ*', 'BEZ-HL', 'BEZ-NC', 'BEZ-TL', 'CC', 'CC-HL', 'CC-NC', 'CC-TL', 'CC-TL-HL', 'CD', 'CD$', 'CD-HL', 'CD-NC', 'CD-TL', 'CD-TL-HL', 'CS', 'CS-HL', 'CS-NC', 'CS-TL', 'DO', 'DO*', 'DO*-HL', 'DO+PPSS', 'DO-HL', 'DO-NC', 'DO-TL', 'DOD', 'DOD*', 'DOD*-TL', 'DOD-NC', 'DOZ', 'DOZ*', 'DOZ*-TL', 'DOZ-HL', 'DOZ-TL', 'DT', 'DT$', 'DT+BEZ', 'DT+BEZ-NC', 'DT+MD', 'DT-HL', 'DT-NC', 'DT-TL', 'DTI', 'DTI-HL', 'DTI-TL', 'DTS', 'DTS+BEZ', 'DTS-HL', 'DTX', 'EX', 'EX+BEZ', 'EX+HVD', 'EX+HVZ', 'EX+MD', 'EX-HL', 'EX-NC', 'FW-*', 'FW-*-TL', 'FW-AT', 'FW-AT+NN-TL', 'FW-AT+NP-TL', 'FW-AT-HL', 'FW-AT-TL', 'FW-BE', 'FW-BER', 'FW-BEZ', 'FW-CC', 'FW-CC-TL', 'FW-CD', 'FW-CD-TL', 'FW-CS', 'FW-DT', 'FW-DT+BEZ', 'FW-DTS', 'FW-HV', 'FW-IN', 'FW-IN+AT', 'FW-IN+AT-T', 'FW-IN+AT-TL', 'FW-IN+NN', 'FW-IN+NN-TL', 'FW-IN+NP-TL', 'FW-IN-TL', 'FW-JJ', 'FW-JJ-NC', 'FW-JJ-TL', 'FW-JJR', 'FW-JJT', 'FW-NN', 'FW-NN$', 'FW-NN$-TL', 'FW-NN-NC', 'FW-NN-TL', 'FW-NN-TL-NC', 'FW-NNS', 'FW-NNS-NC', 'FW-NNS-TL', 'FW-NP', 'FW-NP-TL', 'FW-NPS', 'FW-NPS-TL', 'FW-NR', 'FW-NR-TL', 'FW-OD-NC', 'FW-OD-TL', 'FW-PN', 'FW-PP$', 'FW-PP$-NC', 'FW-PP$-TL', 'FW-PPL', 'FW-PPL+VBZ', 'FW-PPO', 'FW-PPO+IN', 'FW-PPS', 'FW-PPSS', 'FW-PPSS+HV', 'FW-QL', 'FW-RB', 'FW-RB+CC', 'FW-RB-TL', 'FW-TO+VB', 'FW-UH', 'FW-UH-NC', 'FW-UH-TL', 'FW-VB', 'FW-VB-NC', 'FW-VB-TL', 'FW-VBD', 'FW-VBD-TL', 'FW-VBG', 'FW-VBG-TL', 'FW-VBN', 'FW-VBZ', 'FW-WDT', 'FW-WPO', 'FW-WPS', 'HV', 'HV*', 'HV+TO', 'HV-HL', 'HV-NC', 'HV-TL', 'HVD', 'HVD*', 'HVD-HL', 'HVG', 'HVG-HL', 'HVN', 'HVZ', 'HVZ*', 'HVZ-NC', 'HVZ-TL', 'IN', 'IN+IN', 'IN+PPO', 'IN-HL', 'IN-NC', 'IN-TL', 'IN-TL-HL', 'JJ', 'JJ$-TL', 'JJ+JJ-NC', 'JJ-HL', 'JJ-NC', 'JJ-TL', 'JJ-TL-HL', 'JJ-TL-NC', 'JJR', 'JJR+CS', 'JJR-HL', 'JJR-NC', 'JJR-TL', 'JJS', 'JJS-HL', 'JJS-TL', 'JJT', 'JJT-HL', 'JJT-NC', 'JJT-TL', 'MD', 'MD*', 'MD*-HL', 'MD+HV', 'MD+PPSS', 'MD+TO', 'MD-HL', 'MD-NC', 'MD-TL', 'NIL', 'NN', 'NN$', 'NN$-HL', 'NN$-TL', 'NN+BEZ', 'NN+BEZ-TL', 'NN+HVD-TL', 'NN+HVZ', 'NN+HVZ-TL', 'NN+IN', 'NN+MD', 'NN+NN-NC', 'NN-HL', 'NN-NC', 'NN-TL', 'NN-TL-HL', 'NN-TL-NC', 'NNS', 'NNS$', 'NNS$-HL', 'NNS$-NC', 'NNS$-TL', 'NNS$-TL-HL', 'NNS+MD', 'NNS-HL', 'NNS-NC', 'NNS-TL', 'NNS-TL-HL', 'NNS-TL-NC', 'NP', 'NP$', 'NP$-HL', 'NP$-TL', 'NP+BEZ', 'NP+BEZ-NC', 'NP+HVZ', 'NP+HVZ-NC', 'NP+MD', 'NP-HL', 'NP-NC', 'NP-TL', 'NP-TL-HL', 'NPS', 'NPS$', 'NPS$-HL', 'NPS$-TL', 'NPS-HL', 'NPS-NC', 'NPS-TL', 'NR', 'NR$', 'NR$-TL', 'NR+MD', 'NR-HL', 'NR-NC', 'NR-TL', 'NR-TL-HL', 'NRS', 'NRS-TL', 'OD', 'OD-HL', 'OD-NC', 'OD-TL', 'PN', 'PN$', 'PN+BEZ', 'PN+HVD', 'PN+HVZ', 'PN+MD', 'PN-HL', 'PN-NC', 'PN-TL', 'PP$', 'PP$$', 'PP$-HL', 'PP$-NC', 'PP$-TL', 'PPL', 'PPL-HL', 'PPL-NC', 'PPL-TL', 'PPLS', 'PPO', 'PPO-HL', 'PPO-NC', 'PPO-TL', 'PPS', 'PPS+BEZ', 'PPS+BEZ-HL', 'PPS+BEZ-NC', 'PPS+HVD', 'PPS+HVZ', 'PPS+MD', 'PPS-HL', 'PPS-NC', 'PPS-TL', 'PPSS', 'PPSS+BEM', 'PPSS+BER', 'PPSS+BER-N', 'PPSS+BER-NC', 'PPSS+BER-TL', 'PPSS+BEZ', 'PPSS+BEZ*', 'PPSS+HV', 'PPSS+HV-TL', 'PPSS+HVD', 'PPSS+MD', 'PPSS+MD-NC', 'PPSS+VB', 'PPSS-HL', 'PPSS-NC', 'PPSS-TL', 'QL', 'QL-HL', 'QL-NC', 'QL-TL', 'QLP', 'RB', 'RB$', 'RB+BEZ', 'RB+BEZ-HL', 'RB+BEZ-NC', 'RB+CS', 'RB-HL', 'RB-NC', 'RB-TL', 'RBR', 'RBR+CS', 'RBR-NC', 'RBT', 'RN', 'RP', 'RP+IN', 'RP-HL', 'RP-NC', 'RP-TL', 'TO', 'TO+VB', 'TO-HL', 'TO-NC', 'TO-TL', 'UH', 'UH-HL', 'UH-NC', 'UH-TL', 'VB', 'VB+AT', 'VB+IN', 'VB+JJ-NC', 'VB+PPO', 'VB+RP', 'VB+TO', 'VB+VB-NC', 'VB-HL', 'VB-NC', 'VB-TL', 'VBD', 'VBD-HL', 'VBD-NC', 'VBD-TL', 'VBG', 'VBG+TO', 'VBG-HL', 'VBG-NC', 'VBG-TL', 'VBN', 'VBN+TO', 'VBN-HL', 'VBN-NC', 'VBN-TL', 'VBN-TL-HL', 'VBN-TL-NC', 'VBZ', 'VBZ-HL', 'VBZ-NC', 'VBZ-TL', 'WDT', 'WDT+BER', 'WDT+BER+PP', 'WDT+BEZ', 'WDT+BEZ-HL', 'WDT+BEZ-NC', 'WDT+BEZ-TL', 'WDT+DO+PPS', 'WDT+DOD', 'WDT+HVZ', 'WDT-HL', 'WDT-NC', 'WP$', 'WPO', 'WPO-NC', 'WPO-TL', 'WPS', 'WPS+BEZ', 'WPS+BEZ-NC', 'WPS+BEZ-TL', 'WPS+HVD', 'WPS+HVZ', 'WPS+MD', 'WPS-HL', 'WPS-NC', 'WPS-TL', 'WQL', 'WQL-TL', 'WRB', 'WRB+BER', 'WRB+BEZ', 'WRB+BEZ-TL', 'WRB+DO', 'WRB+DOD', 'WRB+DOD*', 'WRB+DOZ', 'WRB+IN', 'WRB+MD', 'WRB-HL', 'WRB-NC', 'WRB-TL', '``']\n"
          ]
        }
      ],
      "source": [
        "print(sorted(set([tag for (word, tag) in nltk.corpus.brown.tagged_words()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2**\n",
        "\n",
        "##Recall the example of a bigram tagger which encountered a word it hadn't seen during training, and tagged the rest of the sentence as None. \n",
        "\n",
        "##It is possible for a bigram tagger to fail part way through a sentence even if it contains no unseen words (even if the sentence was used during training). \n",
        "\n",
        "##In what circumstance can this happen? Can you write a program to find some examples of this?"
      ],
      "metadata": {
        "id": "avoQ6OLhM1FJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "タグ付けの際に、学習対象の単語であっても、学習時と異なるコンテキストで与えられた場合には、その単語は認識されずBigram Taggerはそれ以降の単語の認識にも連鎖的に失敗します。\n",
        "\n",
        "このようにBigram Taggerには、一度単語を認識できなかったとき、それ以降の単語は連鎖的に認識されなくなるという性質があります。\n",
        "\n",
        "よって、タグのペアに認識されなかった単語(unknown)を含むとき、残りのタグのペアは直前の認識されなかったタグのペアをもとに推測されるので、結果的に問題文で指摘された状況が起こります。"
      ],
      "metadata": {
        "id": "oiuKzduI3zVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "\n",
        "training_set = brown_tagged_sents[0]\n",
        "training_sentence = brown.sents()[0]\n",
        "\n",
        "shuffled_sentence = sorted(training_sentence)\n",
        "\n",
        "bigram_training = list(nltk.bigrams(shuffled_sentence))\n",
        "bigram_shuffled = list(nltk.bigrams(training_sentence))\n",
        "\n",
        "# Program to find examples of this would pull out all the extant bigrams \n",
        "# -containing a particular word in the training set. And then it would do the same for the testing set. \n",
        "# Subtract the training set from the testing set. \n",
        "# If anything remains, it is a bigram context that is not accounted for.\n",
        "\n",
        "def test_unknown_contexts(corpus_one, corpus_two):\n",
        "    #tests to see if two inputted sentences are identical\n",
        "    if list(set(corpus_one)-set(corpus_two)):\n",
        "        print(\"There are unknown contexts\")\n",
        "    else:\n",
        "        print(\"All contexts are known\")\n",
        "\n",
        "test_unknown_contexts(bigram_training, bigram_shuffled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01I_Meou2fC5",
        "outputId": "0717b420-984c-4334-92b6-190b473d1fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are unknown contexts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 問題２の解答\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from random import shuffle\n",
        "from nltk.tag import BigramTagger\n",
        "\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "\n",
        "train_sents = brown_tagged_sents[:size]\n",
        "test_sents = brown.sents(categories='news')\n",
        "\n",
        "bigram_tagger = nltk.BigramTagger(train_sents)      # training process\n",
        "\n",
        "print(\"同じコンテキスト\" + str(bigram_tagger.tag(test_sents[2007])))          # the words seen during training comes with same contexts\n",
        "print(\"違うコンテキスト\" + str(bigram_tagger.tag(sorted(test_sents[2007]))))  # the words seen during training comes with different context\n",
        "#　異なるコンテキスト（ソートを用いた）で単語を与えた場合、連鎖的に単語の認識に失敗している"
      ],
      "metadata": {
        "id": "X0BadhhGM0BO",
        "outputId": "2112d70a-6fc7-4477-fbfc-b623b9ac5805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "同じコンテキスト[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n",
            "違うコンテキスト[(',', ','), ('.', None), ('Various', None), ('apartments', None), ('are', None), ('being', None), ('direct', None), ('entrance', None), ('floor', None), ('ground', None), ('is', None), ('of', None), ('of', None), ('on', None), ('so', None), ('terrace', None), ('that', None), ('the', None), ('the', None), ('the', None), ('type', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 3**\n",
        "\n",
        "Preprocess the Brown News data by replacing low frequency words with UNK, but leaving the tags untouched. Now train and evaluate a bigram tagger on this data. How much does this help? What is the contribution of the unigram tagger and default tagger now?"
      ],
      "metadata": {
        "id": "USRwaLmTYqs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "unigram taggerとdefault taggerは、bigram taggerの精度を向上させるという点で非常に役立ちます。\n",
        "\n",
        "bigram taggerがｎ番目とn-1番目の単語をタグ付けする場合、文の最初の単語がコンテキストを持たないことによって生じる問題は、unigram taggerとdefault taggerを補助的に用いることで解決されます。"
      ],
      "metadata": {
        "id": "nxJiyI6yAjOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 問題３の解答\n",
        "\n",
        "from nltk.tag import BigramTagger\n",
        "from nltk.corpus import brown\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# 各単語の頻度表を取得する\n",
        "brown_words = brown.words(categories='news')\n",
        "fdist = FreqDist(word.lower() for word in brown_words)\n",
        "\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "size = int(len(brown_tagged_sents) * 0.9)\n",
        "\n",
        "# テスト用のデータを用意する\n",
        "test_sents = brown_tagged_sents[:size]\n",
        "\n",
        "# 学習用のデータを用意する\n",
        "# 頻度の低い単語はUNKで置き換える\n",
        "train_sents = []\n",
        "for sent in brown_tagged_sents[:size]:\n",
        "  train_sent = []\n",
        "  for (word, tag) in sent:\n",
        "    if fdist[word.lower()] <= 3:  # 頻度が３以下のときはUNKで置き換える 　頻度の値は適宜変更する\n",
        "      word = 'UNK'\n",
        "    train_sent.append((word, tag))\n",
        "  train_sents.append(train_sent)\n",
        "\n",
        "print(brown_tagged_sents[2005])\n",
        "print(train_sents[2005])\n",
        "\n",
        "t0 = nltk.DefaultTagger('NN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "bigram_tagger = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "\n",
        "# Bigram Tagger 単体テスト用\n",
        "#bigram_tagger = nltk.BigramTagger(train_sents)\n",
        "\n",
        "bigram_tagger.evaluate(test_sents)\n",
        "\n",
        "# Bigram Tagger 単体だと精度は20%前後になる\n",
        "# 頻度が低い単語をUNKで置き換えた上で、Default Tagger と Unigram Tagger と Bigram Tagger を組み合わせると８０％前後の精度に落ち着く"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtlgTA2aZMhr",
        "outputId": "dde9210e-b7a9-4fdf-a0a6-4d0c754e0dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sixty-seven', 'CD'), ('living', 'VBG'), ('units', 'NNS'), ('are', 'BER'), ('being', 'BEG'), ('added', 'VBN'), ('to', 'IN'), ('the', 'AT'), ('165-unit', 'JJ'), ('Harbor', 'NN-TL'), ('View', 'NN-TL'), ('Apartments', 'NNS-TL'), ('in', 'IN'), ('the', 'AT'), ('Cherry', 'NN-TL'), ('Hill', 'NN-TL'), ('section', 'NN'), ('.', '.')]\n",
            "[('UNK', 'CD'), ('living', 'VBG'), ('units', 'NNS'), ('are', 'BER'), ('being', 'BEG'), ('added', 'VBN'), ('to', 'IN'), ('the', 'AT'), ('UNK', 'JJ'), ('UNK', 'NN-TL'), ('View', 'NN-TL'), ('Apartments', 'NNS-TL'), ('in', 'IN'), ('the', 'AT'), ('UNK', 'NN-TL'), ('Hill', 'NN-TL'), ('section', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8583643574419195"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}