{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**A05 Information Extraction**"
      ],
      "metadata": {
        "id": "6HwJdUIAXIsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "jUjJjnb5ZBpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**問題 １**\n",
        "\n",
        "###問題文\n",
        "\n",
        "Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
      ],
      "metadata": {
        "id": "xIsV1qg8XCf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iJZtSadW_Aq"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "grammar1 = r\"\"\"\n",
        "  NP: {<DT>?<CD>?<JJ.*>*<NN.*>+}\n",
        "\"\"\"\n",
        "cp1 = nltk.RegexpParser(grammar1)\n",
        "\n",
        "test1 = pos_tag(word_tokenize(\"There are many researchers on Artificial Intelligence in the lab.\"))\n",
        "test2 = pos_tag(word_tokenize(\"The package is going to be shipped in two weeks.\"))\n",
        "test3 = pos_tag(word_tokenize(\"Both new positions proposed to me caught my attention seriously.\"))\n",
        "print(cp1.parse(test1))\n",
        "print(cp1.parse(test2))\n",
        "print(cp1.parse(test3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "(S\n",
        "  There/EX\n",
        "  are/VBP\n",
        "  (NP many/JJ researchers/NNS)\n",
        "  on/IN\n",
        "  (NP Artificial/JJ Intelligence/NN)\n",
        "  in/IN\n",
        "  (NP the/DT lab/NN)\n",
        "  ./.)\n",
        "```\n",
        "\n",
        "```\n",
        "(S\n",
        "  (NP The/DT package/NN)\n",
        "  is/VBZ\n",
        "  going/VBG\n",
        "  to/TO\n",
        "  be/VB\n",
        "  shipped/VBN\n",
        "  in/IN\n",
        "  (NP two/CD weeks/NNS)\n",
        "  ./.)\n",
        "```\n",
        "\n",
        "```  \n",
        "(S\n",
        "  (NP Both/DT new/JJ positions/NNS)\n",
        "  proposed/VBN\n",
        "  to/TO\n",
        "  me/PRP\n",
        "  caught/VB\n",
        "  my/PRP$\n",
        "  (NP attention/NN)\n",
        "  seriously/RB\n",
        "  ./.)\n",
        "```"
      ],
      "metadata": {
        "id": "nQZzw2xjBrXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**問題 ２**\n",
        "\n",
        "###問題文\n",
        "\n",
        "Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
      ],
      "metadata": {
        "id": "-gz-i3AEW-zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "grammar2 = r\"\"\"\n",
        "  NP:\n",
        "      {<DT>?<CD>?<JJ.*>*<NN.*>+<VBG><NN.*>+} # VBGがNNの前に来る場合  \n",
        "      {<DT>?<CD>?<JJ.*>*<VBG><NN.*>+}        # VBGがNNの後に来る場合\n",
        "      {<DT>?<CD>?<JJ.*>*<NN.*>+}             # VBGが含まれない場合\n",
        "\"\"\"\n",
        "\n",
        "cp2 = nltk.RegexpParser(grammar2)\n",
        "test1 = pos_tag(word_tokenize(\"the receiving end\"))\n",
        "test2 = pos_tag(word_tokenize(\"I heard that the assistant managing editor has fired.\"))\n",
        "test3 = pos_tag(word_tokenize(\"Nowadays virtual currency is one of the booming Internet businesses.\"))\n",
        "print(cp2.parse(test1))\n",
        "print(cp2.parse(test2))\n",
        "print(cp2.parse(test3))"
      ],
      "metadata": {
        "id": "OHt2o00mX7hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "(S (NP the/DT receiving/VBG end/NN))\n",
        "```\n",
        "\n",
        "```\n",
        "(S\n",
        "  I/PRP\n",
        "  heard/VBD\n",
        "  that/IN\n",
        "  (NP the/DT assistant/NN managing/VBG editor/NN)\n",
        "  has/VBZ\n",
        "  fired/VBN\n",
        "  ./.)\n",
        "```\n",
        "\n",
        "```\n",
        "(S\n",
        "  Nowadays/RB\n",
        "  (NP virtual/JJ currency/NN)\n",
        "  is/VBZ\n",
        "  one/CD\n",
        "  of/IN\n",
        "  (NP the/DT booming/JJ Internet/NNP businesses/NNS)\n",
        "  ./.)\n",
        "```\n",
        "\n",
        "テスト３に含まれるboomingはVBGのように思われるが、実際には形容詞(JJ)である。grammar2がこのことを正しく識別できるていることから、grammar2はVBGとingが末尾に付く形容詞を区別する能力があることがわかる。\n",
        "\n",
        "残りのテストセットに対してもVBGを含む場合と含まない場合ともに正しく認識できていることが見て取れる。"
      ],
      "metadata": {
        "id": "Rpbzf5Km3rUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**問題 ３**\n",
        "\n",
        "###問題文\n",
        "\n",
        "Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will produce errors.)"
      ],
      "metadata": {
        "id": "gocF6vwepvAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###a. Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure."
      ],
      "metadata": {
        "id": "egaXqUSZ57KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 自作のchunkerに対して、評価作業を行う。\n",
        "\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import conll2000\n",
        "\n",
        "# 自作chunkerの定義\n",
        "grammar3 = r\"\"\"\n",
        "  NP:\n",
        "      {<DT|PRP\\$|POS>?<CD>?<JJ.*>*<NN.*>+<VBG><NN.*>+}\n",
        "      {<DT|PRP\\$|POS>?<CD>?<JJ.*>*<VBG><NN.*>+}\n",
        "      {<DT|PRP\\$|POS>?<CD>?<JJ.*>*<NN.*|PRP>+}\n",
        "\"\"\"\n",
        "\n",
        "cp3 = nltk.RegexpParser(grammar3)\n",
        "\n",
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "print(cp3.evaluate(test_sents[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2JmaKW7wPNP",
        "outputId": "a67f9407-539e-44e0-b5c4-5645030cf6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  84.9%%\n",
            "    Precision:     82.6%%\n",
            "    Recall:        76.0%%\n",
            "    F-Measure:     79.2%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*b*. Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker. Discuss."
      ],
      "metadata": {
        "id": "OQ6PD3Ht51wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import *\n",
        "from nltk.chunk.util import *\n",
        "from nltk.chunk.regexp import *\n",
        "from nltk import Tree\n",
        "\n",
        "chunkscore = ChunkScore()\n",
        "for sent in  test_sents[:4]:\n",
        "  unchunked_sent = sent.flatten()\n",
        "  cp3_sent = cp3.parse(unchunked_sent)\n",
        "  print(sent)\n",
        "  print(cp3_sent)\n",
        "\n",
        "  chunkscore.score(sent, cp3_sent)\n",
        "  print(chunkscore)\n",
        "  print(\"Missed \" + str(chunkscore.missed()))\n",
        "  print(\"Incorrect \" + str(chunkscore.incorrect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnvEWpyWxihk",
        "outputId": "5fd7a8ed-ecba-4ae4-dfe9-97a34a2f5795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP Rockwell/NNP International/NNP Corp./NNP)\n",
            "  (NP 's/POS Tulsa/NNP unit/NN)\n",
            "  said/VBD\n",
            "  (NP it/PRP)\n",
            "  signed/VBD\n",
            "  (NP a/DT tentative/JJ agreement/NN)\n",
            "  extending/VBG\n",
            "  (NP its/PRP$ contract/NN)\n",
            "  with/IN\n",
            "  (NP Boeing/NNP Co./NNP)\n",
            "  to/TO\n",
            "  provide/VB\n",
            "  (NP structural/JJ parts/NNS)\n",
            "  for/IN\n",
            "  (NP Boeing/NNP)\n",
            "  (NP 's/POS 747/CD jetliners/NNS)\n",
            "  ./.)\n",
            "(S\n",
            "  (NP Rockwell/NNP International/NNP Corp./NNP)\n",
            "  (NP 's/POS Tulsa/NNP unit/NN)\n",
            "  said/VBD\n",
            "  (NP it/PRP)\n",
            "  signed/VBD\n",
            "  (NP a/DT tentative/JJ agreement/NN)\n",
            "  extending/VBG\n",
            "  (NP its/PRP$ contract/NN)\n",
            "  with/IN\n",
            "  (NP Boeing/NNP Co./NNP)\n",
            "  to/TO\n",
            "  provide/VB\n",
            "  (NP structural/JJ parts/NNS)\n",
            "  for/IN\n",
            "  (NP Boeing/NNP)\n",
            "  (NP 's/POS 747/CD jetliners/NNS)\n",
            "  ./.)\n",
            "ChunkParse score:\n",
            "    IOB Accuracy: 100.0%%\n",
            "    Precision:    100.0%%\n",
            "    Recall:       100.0%%\n",
            "    F-Measure:    100.0%%\n",
            "Missed []\n",
            "Incorrect []\n",
            "(S\n",
            "  (NP Rockwell/NNP)\n",
            "  said/VBD\n",
            "  (NP the/DT agreement/NN)\n",
            "  calls/VBZ\n",
            "  for/IN\n",
            "  (NP it/PRP)\n",
            "  to/TO\n",
            "  supply/VB\n",
            "  (NP 200/CD additional/JJ so-called/JJ shipsets/NNS)\n",
            "  for/IN\n",
            "  (NP the/DT planes/NNS)\n",
            "  ./.)\n",
            "(S\n",
            "  (NP Rockwell/NNP)\n",
            "  said/VBD\n",
            "  (NP the/DT agreement/NN)\n",
            "  calls/VBZ\n",
            "  for/IN\n",
            "  (NP it/PRP)\n",
            "  to/TO\n",
            "  supply/VB\n",
            "  (NP 200/CD additional/JJ so-called/JJ shipsets/NNS)\n",
            "  for/IN\n",
            "  (NP the/DT planes/NNS)\n",
            "  ./.)\n",
            "ChunkParse score:\n",
            "    IOB Accuracy: 100.0%%\n",
            "    Precision:    100.0%%\n",
            "    Recall:       100.0%%\n",
            "    F-Measure:    100.0%%\n",
            "Missed []\n",
            "Incorrect []\n",
            "(S\n",
            "  (NP These/DT)\n",
            "  include/VBP\n",
            "  ,/,\n",
            "  among/IN\n",
            "  (NP other/JJ parts/NNS)\n",
            "  ,/,\n",
            "  (NP each/DT jetliner/NN)\n",
            "  (NP 's/POS two/CD major/JJ bulkheads/NNS)\n",
            "  ,/,\n",
            "  (NP a/DT pressure/NN floor/NN)\n",
            "  ,/,\n",
            "  (NP torque/NN box/NN)\n",
            "  ,/,\n",
            "  (NP fixed/VBN leading/VBG edges/NNS)\n",
            "  for/IN\n",
            "  (NP the/DT wings/NNS)\n",
            "  and/CC\n",
            "  (NP an/DT aft/JJ keel/NN beam/NN)\n",
            "  ./.)\n",
            "(S\n",
            "  These/DT\n",
            "  include/VBP\n",
            "  ,/,\n",
            "  among/IN\n",
            "  (NP other/JJ parts/NNS)\n",
            "  ,/,\n",
            "  (NP each/DT jetliner/NN)\n",
            "  (NP 's/POS two/CD major/JJ bulkheads/NNS)\n",
            "  ,/,\n",
            "  (NP a/DT pressure/NN floor/NN)\n",
            "  ,/,\n",
            "  (NP torque/NN box/NN)\n",
            "  ,/,\n",
            "  fixed/VBN\n",
            "  (NP leading/VBG edges/NNS)\n",
            "  for/IN\n",
            "  (NP the/DT wings/NNS)\n",
            "  and/CC\n",
            "  (NP an/DT aft/JJ keel/NN beam/NN)\n",
            "  ./.)\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  96.2%%\n",
            "    Precision:     95.5%%\n",
            "    Recall:        91.3%%\n",
            "    F-Measure:     93.3%%\n",
            "Missed [ImmutableTree('NP', [('These', 'DT')]), ImmutableTree('NP', [('fixed', 'VBN'), ('leading', 'VBG'), ('edges', 'NNS')])]\n",
            "Incorrect [ImmutableTree('NP', [('leading', 'VBG'), ('edges', 'NNS')])]\n",
            "(S\n",
            "  Under/IN\n",
            "  (NP the/DT existing/VBG contract/NN)\n",
            "  ,/,\n",
            "  (NP Rockwell/NNP)\n",
            "  said/VBD\n",
            "  ,/,\n",
            "  (NP it/PRP)\n",
            "  has/VBZ\n",
            "  already/RB\n",
            "  delivered/VBN\n",
            "  (NP 793/CD)\n",
            "  of/IN\n",
            "  (NP the/DT shipsets/NNS)\n",
            "  to/TO\n",
            "  (NP Boeing/NNP)\n",
            "  ./.)\n",
            "(S\n",
            "  Under/IN\n",
            "  (NP the/DT existing/VBG contract/NN)\n",
            "  ,/,\n",
            "  (NP Rockwell/NNP)\n",
            "  said/VBD\n",
            "  ,/,\n",
            "  (NP it/PRP)\n",
            "  has/VBZ\n",
            "  already/RB\n",
            "  delivered/VBN\n",
            "  793/CD\n",
            "  of/IN\n",
            "  (NP the/DT shipsets/NNS)\n",
            "  to/TO\n",
            "  (NP Boeing/NNP)\n",
            "  ./.)\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  95.9%%\n",
            "    Precision:     96.3%%\n",
            "    Recall:        89.7%%\n",
            "    F-Measure:     92.9%%\n",
            "Missed [ImmutableTree('NP', [('These', 'DT')]), ImmutableTree('NP', [('fixed', 'VBN'), ('leading', 'VBG'), ('edges', 'NNS')]), ImmutableTree('NP', [('793', 'CD')])]\n",
            "Incorrect [ImmutableTree('NP', [('leading', 'VBG'), ('edges', 'NNS')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回自作したchunkerは、どの評価手法に対しても90%程度の結果を残していることから、大抵の場合は上手く動作していることがわかる。\n",
        "\n",
        "しかし、複雑な名詞句に対しては正しくchunk化を行えていないことが、chunkscore.missed()関数やchunkscore.incorrect()関数の出力から認められる。\n",
        "\n",
        "具体的には、名詞句内でVBNがVBGとともに現れたケースや複数の名詞をコンマと接続詞を用いて一つの名詞句にまとめたケースなどが挙げられる。\n",
        "\n",
        "今後、さらなるchunk化能力の向上のためには上で上げたような複雑な名詞句に対応する仕組みを考えてあげる必要があるだろう。"
      ],
      "metadata": {
        "id": "J94mlBWn6cMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter."
      ],
      "metadata": {
        "id": "2-x6mvgi-hz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline chunkerに対して、評価作業を行う。\n",
        "\n",
        "grammar_baseline = r\"NP: {<[CDJNP].*>+}\"\n",
        "cp_bc = nltk.RegexpParser(grammar_baseline)\n",
        "print(cp_bc.evaluate(test_sents[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xZBgqP3rUrt",
        "outputId": "ffaf22a8-9cdb-4b19-908b-bfc947755947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  83.9%%\n",
            "    Precision:     62.1%%\n",
            "    Recall:        58.2%%\n",
            "    F-Measure:     60.1%%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**baseline chunkerのパフォーマンス**\n",
        "```\n",
        "    IOB Accuracy:  83.9%%\n",
        "    Precision:     62.1%%\n",
        "    Recall:        58.2%%\n",
        "    F-Measure:     60.1%%\n",
        "```\n",
        "\n",
        "**自作のchunkerのパフォーマンス**\n",
        "```\n",
        "    IOB Accuracy:  84.9%%\n",
        "    Precision:     82.6%%\n",
        "    Recall:        76.0%%\n",
        "    F-Measure:     79.2%%\n",
        "```\n",
        "\n",
        "それぞれのchunkerを評価した結果についてまとめた上の二つの表からは、４つのある評価指標のいずれの場合においても上回っていることが見て取れる。\n",
        "\n",
        "特にIOB Accuracy以外の３つの評価指標においてはbaseline chunkerに比べて10%から20%の大幅なchunking能力の向上を果たしている。"
      ],
      "metadata": {
        "id": "e9vm7TjO-3n3"
      }
    }
  ]
}